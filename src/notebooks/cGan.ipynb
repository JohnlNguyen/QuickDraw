{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs('images', exist_ok=True)\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "channels = 1\n",
    "img_size = 28\n",
    "n_classes = 10\n",
    "latent_dim = 100\n",
    "batch_size = 32\n",
    "learning_rate = .0002\n",
    "b1 = .5\n",
    "b2 = .999\n",
    "sample_interval = 400\n",
    "n_epochs = 2\n",
    "img_shape = (channels, img_size, img_size)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(n_classes, n_classes)\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [  nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim+n_classes, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Embedding(n_classes, n_classes)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_classes + int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "# Loss functions\n",
    "adversarial_loss = torch.nn.MSELoss()\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    auxiliary_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(b1, b2))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "\n",
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    # Sample noise\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row**2, latent_dim))))\n",
    "    # Get labels ranging from 0 to n_classes for n rows\n",
    "    labels = np.array([1 for _ in range(n_row) for num in range(n_row)])\n",
    "    labels = Variable(LongTensor(labels))\n",
    "    gen_imgs = generator(z, labels)\n",
    "    save_image(gen_imgs.data, 'images/%d.png' % batches_done, nrow=n_row, normalize=True)\n",
    "    \n",
    "\n",
    "def save_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['saxophone',\n",
    "            'raccoon',\n",
    "            'piano',\n",
    "            'panda',\n",
    "            'leg',\n",
    "            'headphones',\n",
    "            'ceiling_fan',\n",
    "            'bed',\n",
    "            'basket',\n",
    "            'aircraft_carrier']\n",
    "    \n",
    "class QuickDrawDataset(Dataset):\n",
    "    \"\"\"Quick Draw dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, label, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = self.get_all_classes(1000)\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "    \n",
    "    def get_all_classes(self, sample_per_class=100):\n",
    "        all_classes = []\n",
    "        for i, label in enumerate(classes):\n",
    "            img = np.apply_along_axis(self.__reshape_row, 1, np.load('../data/%s.npy' % label))\n",
    "            all_classes.extend([(row, i) for row in img[np.random.choice(len(img), sample_per_class)]])\n",
    "        return all_classes\n",
    "    \n",
    "    def __reshape_row(self, row):\n",
    "        return np.reshape(row, (28, 28))\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data_frame[idx][0]\n",
    "        label = self.data_frame[idx][1]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = QuickDrawDataset(label='panda', transform=transforms.Compose([\n",
    "                        transforms.Resize(img_size),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "dataloader = DataLoader(data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/1] [Batch 0/313] [D loss: 26.027779] [G loss: 1.033194]\n",
      "[Epoch 0/1] [Batch 1/313] [D loss: 11.412572] [G loss: 1.035743]\n",
      "[Epoch 0/1] [Batch 2/313] [D loss: 15.312386] [G loss: 1.040489]\n",
      "[Epoch 0/1] [Batch 3/313] [D loss: 11.950084] [G loss: 1.035176]\n",
      "[Epoch 0/1] [Batch 4/313] [D loss: 16.428133] [G loss: 1.033281]\n",
      "[Epoch 0/1] [Batch 5/313] [D loss: 19.813412] [G loss: 1.046757]\n",
      "[Epoch 0/1] [Batch 6/313] [D loss: 24.196918] [G loss: 1.036465]\n",
      "[Epoch 0/1] [Batch 7/313] [D loss: 11.250469] [G loss: 1.040988]\n",
      "[Epoch 0/1] [Batch 8/313] [D loss: 10.368031] [G loss: 1.047000]\n",
      "[Epoch 0/1] [Batch 9/313] [D loss: 18.838497] [G loss: 1.043866]\n",
      "[Epoch 0/1] [Batch 10/313] [D loss: 13.280270] [G loss: 1.039758]\n",
      "[Epoch 0/1] [Batch 11/313] [D loss: 14.582309] [G loss: 1.036922]\n",
      "[Epoch 0/1] [Batch 12/313] [D loss: 13.275769] [G loss: 1.042121]\n",
      "[Epoch 0/1] [Batch 13/313] [D loss: 14.544820] [G loss: 1.031550]\n",
      "[Epoch 0/1] [Batch 14/313] [D loss: 25.511530] [G loss: 1.035457]\n",
      "[Epoch 0/1] [Batch 15/313] [D loss: 12.240170] [G loss: 1.037768]\n",
      "[Epoch 0/1] [Batch 16/313] [D loss: 11.998482] [G loss: 1.039586]\n",
      "[Epoch 0/1] [Batch 17/313] [D loss: 14.115269] [G loss: 1.036806]\n",
      "[Epoch 0/1] [Batch 18/313] [D loss: 18.322721] [G loss: 1.040346]\n",
      "[Epoch 0/1] [Batch 19/313] [D loss: 17.464001] [G loss: 1.040683]\n",
      "[Epoch 0/1] [Batch 20/313] [D loss: 19.319471] [G loss: 1.040300]\n",
      "[Epoch 0/1] [Batch 21/313] [D loss: 29.840023] [G loss: 1.031210]\n",
      "[Epoch 0/1] [Batch 22/313] [D loss: 20.609289] [G loss: 1.039245]\n",
      "[Epoch 0/1] [Batch 23/313] [D loss: 17.811775] [G loss: 1.035721]\n",
      "[Epoch 0/1] [Batch 24/313] [D loss: 22.232409] [G loss: 1.043515]\n",
      "[Epoch 0/1] [Batch 25/313] [D loss: 16.267920] [G loss: 1.032971]\n",
      "[Epoch 0/1] [Batch 26/313] [D loss: 16.324896] [G loss: 1.033964]\n",
      "[Epoch 0/1] [Batch 27/313] [D loss: 18.700924] [G loss: 1.034699]\n",
      "[Epoch 0/1] [Batch 28/313] [D loss: 18.035618] [G loss: 1.040538]\n",
      "[Epoch 0/1] [Batch 29/313] [D loss: 22.860649] [G loss: 1.037568]\n",
      "[Epoch 0/1] [Batch 30/313] [D loss: 16.585194] [G loss: 1.037695]\n",
      "[Epoch 0/1] [Batch 31/313] [D loss: 22.462242] [G loss: 1.029439]\n",
      "[Epoch 0/1] [Batch 32/313] [D loss: 26.376740] [G loss: 1.035606]\n",
      "[Epoch 0/1] [Batch 33/313] [D loss: 21.658051] [G loss: 1.036144]\n",
      "[Epoch 0/1] [Batch 34/313] [D loss: 34.293930] [G loss: 1.034138]\n",
      "[Epoch 0/1] [Batch 35/313] [D loss: 17.983730] [G loss: 1.033084]\n",
      "[Epoch 0/1] [Batch 36/313] [D loss: 21.717869] [G loss: 1.036461]\n",
      "[Epoch 0/1] [Batch 37/313] [D loss: 20.050549] [G loss: 1.035176]\n",
      "[Epoch 0/1] [Batch 38/313] [D loss: 24.904734] [G loss: 1.045439]\n",
      "[Epoch 0/1] [Batch 39/313] [D loss: 22.789795] [G loss: 1.036422]\n",
      "[Epoch 0/1] [Batch 40/313] [D loss: 18.382853] [G loss: 1.036904]\n",
      "[Epoch 0/1] [Batch 41/313] [D loss: 17.954697] [G loss: 1.036159]\n",
      "[Epoch 0/1] [Batch 42/313] [D loss: 18.565407] [G loss: 1.040287]\n",
      "[Epoch 0/1] [Batch 43/313] [D loss: 29.546955] [G loss: 1.028400]\n",
      "[Epoch 0/1] [Batch 44/313] [D loss: 32.329578] [G loss: 1.032747]\n",
      "[Epoch 0/1] [Batch 45/313] [D loss: 30.603065] [G loss: 1.036296]\n",
      "[Epoch 0/1] [Batch 46/313] [D loss: 22.528513] [G loss: 1.037388]\n",
      "[Epoch 0/1] [Batch 47/313] [D loss: 29.794525] [G loss: 1.039867]\n",
      "[Epoch 0/1] [Batch 48/313] [D loss: 20.447456] [G loss: 1.031023]\n",
      "[Epoch 0/1] [Batch 49/313] [D loss: 27.162390] [G loss: 1.036920]\n",
      "[Epoch 0/1] [Batch 50/313] [D loss: 27.987770] [G loss: 1.031937]\n",
      "[Epoch 0/1] [Batch 51/313] [D loss: 17.883190] [G loss: 1.035556]\n",
      "[Epoch 0/1] [Batch 52/313] [D loss: 21.102163] [G loss: 1.033315]\n",
      "[Epoch 0/1] [Batch 53/313] [D loss: 23.687698] [G loss: 1.040865]\n",
      "[Epoch 0/1] [Batch 54/313] [D loss: 28.623146] [G loss: 1.036403]\n",
      "[Epoch 0/1] [Batch 55/313] [D loss: 22.568302] [G loss: 1.036360]\n",
      "[Epoch 0/1] [Batch 56/313] [D loss: 13.935178] [G loss: 1.041545]\n",
      "[Epoch 0/1] [Batch 57/313] [D loss: 24.381296] [G loss: 1.036526]\n",
      "[Epoch 0/1] [Batch 58/313] [D loss: 26.801374] [G loss: 1.036009]\n",
      "[Epoch 0/1] [Batch 59/313] [D loss: 30.919027] [G loss: 1.034893]\n",
      "[Epoch 0/1] [Batch 60/313] [D loss: 24.622972] [G loss: 1.040726]\n",
      "[Epoch 0/1] [Batch 61/313] [D loss: 36.439358] [G loss: 1.035278]\n",
      "[Epoch 0/1] [Batch 62/313] [D loss: 38.764786] [G loss: 1.042749]\n",
      "[Epoch 0/1] [Batch 63/313] [D loss: 21.440378] [G loss: 1.035307]\n",
      "[Epoch 0/1] [Batch 64/313] [D loss: 26.415892] [G loss: 1.035462]\n",
      "[Epoch 0/1] [Batch 65/313] [D loss: 24.084183] [G loss: 1.041272]\n",
      "[Epoch 0/1] [Batch 66/313] [D loss: 32.409775] [G loss: 1.035704]\n",
      "[Epoch 0/1] [Batch 67/313] [D loss: 17.164179] [G loss: 1.039256]\n",
      "[Epoch 0/1] [Batch 68/313] [D loss: 33.674236] [G loss: 1.040096]\n",
      "[Epoch 0/1] [Batch 69/313] [D loss: 18.800106] [G loss: 1.035557]\n",
      "[Epoch 0/1] [Batch 70/313] [D loss: 16.411098] [G loss: 1.038797]\n",
      "[Epoch 0/1] [Batch 71/313] [D loss: 27.887302] [G loss: 1.033079]\n",
      "[Epoch 0/1] [Batch 72/313] [D loss: 24.958149] [G loss: 1.042338]\n",
      "[Epoch 0/1] [Batch 73/313] [D loss: 24.307535] [G loss: 1.030996]\n",
      "[Epoch 0/1] [Batch 74/313] [D loss: 32.593597] [G loss: 1.031261]\n",
      "[Epoch 0/1] [Batch 75/313] [D loss: 28.423498] [G loss: 1.034286]\n",
      "[Epoch 0/1] [Batch 76/313] [D loss: 24.466373] [G loss: 1.037630]\n",
      "[Epoch 0/1] [Batch 77/313] [D loss: 19.104294] [G loss: 1.040881]\n",
      "[Epoch 0/1] [Batch 78/313] [D loss: 19.217859] [G loss: 1.041345]\n",
      "[Epoch 0/1] [Batch 79/313] [D loss: 28.447920] [G loss: 1.032769]\n",
      "[Epoch 0/1] [Batch 80/313] [D loss: 35.623653] [G loss: 1.039703]\n",
      "[Epoch 0/1] [Batch 81/313] [D loss: 40.790237] [G loss: 1.041950]\n",
      "[Epoch 0/1] [Batch 82/313] [D loss: 37.742550] [G loss: 1.034361]\n",
      "[Epoch 0/1] [Batch 83/313] [D loss: 29.831059] [G loss: 1.036634]\n",
      "[Epoch 0/1] [Batch 84/313] [D loss: 30.249718] [G loss: 1.031749]\n",
      "[Epoch 0/1] [Batch 85/313] [D loss: 26.377895] [G loss: 1.040908]\n",
      "[Epoch 0/1] [Batch 86/313] [D loss: 23.340326] [G loss: 1.032197]\n",
      "[Epoch 0/1] [Batch 87/313] [D loss: 31.008966] [G loss: 1.037361]\n",
      "[Epoch 0/1] [Batch 88/313] [D loss: 32.983265] [G loss: 1.037665]\n",
      "[Epoch 0/1] [Batch 89/313] [D loss: 23.333071] [G loss: 1.041838]\n",
      "[Epoch 0/1] [Batch 90/313] [D loss: 23.446329] [G loss: 1.041825]\n",
      "[Epoch 0/1] [Batch 91/313] [D loss: 30.682480] [G loss: 1.041854]\n",
      "[Epoch 0/1] [Batch 92/313] [D loss: 38.045769] [G loss: 1.040111]\n",
      "[Epoch 0/1] [Batch 93/313] [D loss: 33.525669] [G loss: 1.031109]\n",
      "[Epoch 0/1] [Batch 94/313] [D loss: 16.651964] [G loss: 1.037225]\n",
      "[Epoch 0/1] [Batch 95/313] [D loss: 41.608921] [G loss: 1.030439]\n",
      "[Epoch 0/1] [Batch 96/313] [D loss: 14.190503] [G loss: 1.041044]\n",
      "[Epoch 0/1] [Batch 97/313] [D loss: 29.756304] [G loss: 1.036376]\n",
      "[Epoch 0/1] [Batch 98/313] [D loss: 22.013948] [G loss: 1.039297]\n",
      "[Epoch 0/1] [Batch 99/313] [D loss: 32.617157] [G loss: 1.035817]\n",
      "[Epoch 0/1] [Batch 100/313] [D loss: 44.599213] [G loss: 1.041574]\n",
      "[Epoch 0/1] [Batch 101/313] [D loss: 37.479256] [G loss: 1.033887]\n",
      "[Epoch 0/1] [Batch 102/313] [D loss: 25.084106] [G loss: 1.037679]\n",
      "[Epoch 0/1] [Batch 103/313] [D loss: 33.244537] [G loss: 1.030479]\n",
      "[Epoch 0/1] [Batch 104/313] [D loss: 33.094662] [G loss: 1.028790]\n",
      "[Epoch 0/1] [Batch 105/313] [D loss: 19.490788] [G loss: 1.037668]\n",
      "[Epoch 0/1] [Batch 106/313] [D loss: 23.780695] [G loss: 1.037021]\n",
      "[Epoch 0/1] [Batch 107/313] [D loss: 19.480028] [G loss: 1.033397]\n",
      "[Epoch 0/1] [Batch 108/313] [D loss: 26.926842] [G loss: 1.040712]\n",
      "[Epoch 0/1] [Batch 109/313] [D loss: 47.074059] [G loss: 1.035191]\n",
      "[Epoch 0/1] [Batch 110/313] [D loss: 22.790672] [G loss: 1.038095]\n",
      "[Epoch 0/1] [Batch 111/313] [D loss: 25.683884] [G loss: 1.043164]\n",
      "[Epoch 0/1] [Batch 112/313] [D loss: 39.521278] [G loss: 1.038058]\n",
      "[Epoch 0/1] [Batch 113/313] [D loss: 26.057404] [G loss: 1.034822]\n",
      "[Epoch 0/1] [Batch 114/313] [D loss: 43.153427] [G loss: 1.039751]\n",
      "[Epoch 0/1] [Batch 115/313] [D loss: 23.575481] [G loss: 1.038255]\n",
      "[Epoch 0/1] [Batch 116/313] [D loss: 20.877562] [G loss: 1.037849]\n",
      "[Epoch 0/1] [Batch 117/313] [D loss: 30.821184] [G loss: 1.039831]\n",
      "[Epoch 0/1] [Batch 118/313] [D loss: 32.585514] [G loss: 1.033127]\n",
      "[Epoch 0/1] [Batch 119/313] [D loss: 35.588970] [G loss: 1.037324]\n",
      "[Epoch 0/1] [Batch 120/313] [D loss: 21.331308] [G loss: 1.042041]\n",
      "[Epoch 0/1] [Batch 121/313] [D loss: 18.661852] [G loss: 1.042662]\n",
      "[Epoch 0/1] [Batch 122/313] [D loss: 22.200314] [G loss: 1.038246]\n",
      "[Epoch 0/1] [Batch 123/313] [D loss: 38.872108] [G loss: 1.031189]\n",
      "[Epoch 0/1] [Batch 124/313] [D loss: 23.138617] [G loss: 1.031942]\n",
      "[Epoch 0/1] [Batch 125/313] [D loss: 10.665380] [G loss: 1.035049]\n",
      "[Epoch 0/1] [Batch 126/313] [D loss: 7.848625] [G loss: 1.035588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/1] [Batch 127/313] [D loss: 10.988154] [G loss: 1.039900]\n",
      "[Epoch 0/1] [Batch 128/313] [D loss: 8.823195] [G loss: 1.034068]\n",
      "[Epoch 0/1] [Batch 129/313] [D loss: 7.729195] [G loss: 1.040521]\n",
      "[Epoch 0/1] [Batch 130/313] [D loss: 8.649550] [G loss: 1.035308]\n",
      "[Epoch 0/1] [Batch 131/313] [D loss: 12.775069] [G loss: 1.039873]\n",
      "[Epoch 0/1] [Batch 132/313] [D loss: 9.249232] [G loss: 1.029906]\n",
      "[Epoch 0/1] [Batch 133/313] [D loss: 8.919064] [G loss: 1.027646]\n",
      "[Epoch 0/1] [Batch 134/313] [D loss: 10.697927] [G loss: 1.037428]\n",
      "[Epoch 0/1] [Batch 135/313] [D loss: 9.310973] [G loss: 1.041467]\n",
      "[Epoch 0/1] [Batch 136/313] [D loss: 7.430064] [G loss: 1.035313]\n",
      "[Epoch 0/1] [Batch 137/313] [D loss: 6.564551] [G loss: 1.037226]\n",
      "[Epoch 0/1] [Batch 138/313] [D loss: 13.219500] [G loss: 1.036337]\n",
      "[Epoch 0/1] [Batch 139/313] [D loss: 14.457786] [G loss: 1.033307]\n",
      "[Epoch 0/1] [Batch 140/313] [D loss: 9.127892] [G loss: 1.038392]\n",
      "[Epoch 0/1] [Batch 141/313] [D loss: 7.764158] [G loss: 1.037621]\n",
      "[Epoch 0/1] [Batch 142/313] [D loss: 7.422914] [G loss: 1.040740]\n",
      "[Epoch 0/1] [Batch 143/313] [D loss: 8.979299] [G loss: 1.040833]\n",
      "[Epoch 0/1] [Batch 144/313] [D loss: 11.164378] [G loss: 1.033734]\n",
      "[Epoch 0/1] [Batch 145/313] [D loss: 8.335629] [G loss: 1.036277]\n",
      "[Epoch 0/1] [Batch 146/313] [D loss: 10.052777] [G loss: 1.034010]\n",
      "[Epoch 0/1] [Batch 147/313] [D loss: 7.504871] [G loss: 1.038003]\n",
      "[Epoch 0/1] [Batch 148/313] [D loss: 12.700234] [G loss: 1.038275]\n",
      "[Epoch 0/1] [Batch 149/313] [D loss: 10.086127] [G loss: 1.039593]\n",
      "[Epoch 0/1] [Batch 150/313] [D loss: 7.470296] [G loss: 1.035885]\n",
      "[Epoch 0/1] [Batch 151/313] [D loss: 10.336583] [G loss: 1.041130]\n",
      "[Epoch 0/1] [Batch 152/313] [D loss: 5.058568] [G loss: 1.041642]\n",
      "[Epoch 0/1] [Batch 153/313] [D loss: 22.129831] [G loss: 1.041790]\n",
      "[Epoch 0/1] [Batch 154/313] [D loss: 13.253682] [G loss: 1.036809]\n",
      "[Epoch 0/1] [Batch 155/313] [D loss: 5.762409] [G loss: 1.037720]\n",
      "[Epoch 0/1] [Batch 156/313] [D loss: 18.860859] [G loss: 1.033752]\n",
      "[Epoch 0/1] [Batch 157/313] [D loss: 17.185888] [G loss: 1.039106]\n",
      "[Epoch 0/1] [Batch 158/313] [D loss: 19.425962] [G loss: 1.031192]\n",
      "[Epoch 0/1] [Batch 159/313] [D loss: 34.353256] [G loss: 1.039253]\n",
      "[Epoch 0/1] [Batch 160/313] [D loss: 32.935802] [G loss: 1.030608]\n",
      "[Epoch 0/1] [Batch 161/313] [D loss: 21.861950] [G loss: 1.032604]\n",
      "[Epoch 0/1] [Batch 162/313] [D loss: 20.499214] [G loss: 1.030975]\n",
      "[Epoch 0/1] [Batch 163/313] [D loss: 25.214121] [G loss: 1.039351]\n",
      "[Epoch 0/1] [Batch 164/313] [D loss: 34.862411] [G loss: 1.036755]\n",
      "[Epoch 0/1] [Batch 165/313] [D loss: 45.266666] [G loss: 1.030966]\n",
      "[Epoch 0/1] [Batch 166/313] [D loss: 26.401114] [G loss: 1.035533]\n",
      "[Epoch 0/1] [Batch 167/313] [D loss: 21.376801] [G loss: 1.039520]\n",
      "[Epoch 0/1] [Batch 168/313] [D loss: 21.408859] [G loss: 1.038977]\n",
      "[Epoch 0/1] [Batch 169/313] [D loss: 23.620567] [G loss: 1.036374]\n",
      "[Epoch 0/1] [Batch 170/313] [D loss: 27.757315] [G loss: 1.038928]\n",
      "[Epoch 0/1] [Batch 171/313] [D loss: 17.136383] [G loss: 1.038176]\n",
      "[Epoch 0/1] [Batch 172/313] [D loss: 25.990049] [G loss: 1.048561]\n",
      "[Epoch 0/1] [Batch 173/313] [D loss: 14.508031] [G loss: 1.037504]\n",
      "[Epoch 0/1] [Batch 174/313] [D loss: 22.262905] [G loss: 1.036664]\n",
      "[Epoch 0/1] [Batch 175/313] [D loss: 27.411707] [G loss: 1.039128]\n",
      "[Epoch 0/1] [Batch 176/313] [D loss: 21.414684] [G loss: 1.035585]\n",
      "[Epoch 0/1] [Batch 177/313] [D loss: 30.357433] [G loss: 1.041166]\n",
      "[Epoch 0/1] [Batch 178/313] [D loss: 29.489269] [G loss: 1.034624]\n",
      "[Epoch 0/1] [Batch 179/313] [D loss: 34.612976] [G loss: 1.041867]\n",
      "[Epoch 0/1] [Batch 180/313] [D loss: 25.040604] [G loss: 1.031321]\n",
      "[Epoch 0/1] [Batch 181/313] [D loss: 18.171921] [G loss: 1.034721]\n",
      "[Epoch 0/1] [Batch 182/313] [D loss: 22.463121] [G loss: 1.029704]\n",
      "[Epoch 0/1] [Batch 183/313] [D loss: 17.466013] [G loss: 1.037677]\n",
      "[Epoch 0/1] [Batch 184/313] [D loss: 18.924891] [G loss: 1.036251]\n",
      "[Epoch 0/1] [Batch 185/313] [D loss: 28.415665] [G loss: 1.028663]\n",
      "[Epoch 0/1] [Batch 186/313] [D loss: 23.533146] [G loss: 1.032830]\n",
      "[Epoch 0/1] [Batch 187/313] [D loss: 16.734230] [G loss: 1.036515]\n",
      "[Epoch 0/1] [Batch 188/313] [D loss: 19.603848] [G loss: 1.036576]\n",
      "[Epoch 0/1] [Batch 189/313] [D loss: 22.563480] [G loss: 1.036751]\n",
      "[Epoch 0/1] [Batch 190/313] [D loss: 15.363441] [G loss: 1.040115]\n",
      "[Epoch 0/1] [Batch 191/313] [D loss: 13.541813] [G loss: 1.034542]\n",
      "[Epoch 0/1] [Batch 192/313] [D loss: 17.462887] [G loss: 1.035045]\n",
      "[Epoch 0/1] [Batch 193/313] [D loss: 23.726942] [G loss: 1.041331]\n",
      "[Epoch 0/1] [Batch 194/313] [D loss: 15.857192] [G loss: 1.035874]\n",
      "[Epoch 0/1] [Batch 195/313] [D loss: 14.334195] [G loss: 1.036133]\n",
      "[Epoch 0/1] [Batch 196/313] [D loss: 15.033019] [G loss: 1.038939]\n",
      "[Epoch 0/1] [Batch 197/313] [D loss: 15.152951] [G loss: 1.039326]\n",
      "[Epoch 0/1] [Batch 198/313] [D loss: 17.955601] [G loss: 1.032621]\n",
      "[Epoch 0/1] [Batch 199/313] [D loss: 19.814335] [G loss: 1.042393]\n",
      "[Epoch 0/1] [Batch 200/313] [D loss: 19.319180] [G loss: 1.042042]\n",
      "[Epoch 0/1] [Batch 201/313] [D loss: 15.575244] [G loss: 1.032512]\n",
      "[Epoch 0/1] [Batch 202/313] [D loss: 13.301917] [G loss: 1.033715]\n",
      "[Epoch 0/1] [Batch 203/313] [D loss: 12.653299] [G loss: 1.036283]\n",
      "[Epoch 0/1] [Batch 204/313] [D loss: 24.813911] [G loss: 1.036987]\n",
      "[Epoch 0/1] [Batch 205/313] [D loss: 15.430557] [G loss: 1.038170]\n",
      "[Epoch 0/1] [Batch 206/313] [D loss: 20.058195] [G loss: 1.038181]\n",
      "[Epoch 0/1] [Batch 207/313] [D loss: 11.467321] [G loss: 1.030194]\n",
      "[Epoch 0/1] [Batch 208/313] [D loss: 16.140394] [G loss: 1.036208]\n",
      "[Epoch 0/1] [Batch 209/313] [D loss: 21.863819] [G loss: 1.037382]\n",
      "[Epoch 0/1] [Batch 210/313] [D loss: 18.468172] [G loss: 1.040982]\n",
      "[Epoch 0/1] [Batch 211/313] [D loss: 16.204258] [G loss: 1.039413]\n",
      "[Epoch 0/1] [Batch 212/313] [D loss: 13.581859] [G loss: 1.037587]\n",
      "[Epoch 0/1] [Batch 213/313] [D loss: 28.296116] [G loss: 1.039809]\n",
      "[Epoch 0/1] [Batch 214/313] [D loss: 26.349350] [G loss: 1.039315]\n",
      "[Epoch 0/1] [Batch 215/313] [D loss: 18.721939] [G loss: 1.033617]\n",
      "[Epoch 0/1] [Batch 216/313] [D loss: 21.875338] [G loss: 1.035235]\n",
      "[Epoch 0/1] [Batch 217/313] [D loss: 15.219609] [G loss: 1.037611]\n",
      "[Epoch 0/1] [Batch 218/313] [D loss: 25.879473] [G loss: 1.033985]\n",
      "[Epoch 0/1] [Batch 219/313] [D loss: 17.988270] [G loss: 1.048062]\n",
      "[Epoch 0/1] [Batch 220/313] [D loss: 27.108568] [G loss: 1.036185]\n",
      "[Epoch 0/1] [Batch 221/313] [D loss: 20.105782] [G loss: 1.035824]\n",
      "[Epoch 0/1] [Batch 222/313] [D loss: 16.749586] [G loss: 1.043207]\n",
      "[Epoch 0/1] [Batch 223/313] [D loss: 24.053459] [G loss: 1.041890]\n",
      "[Epoch 0/1] [Batch 224/313] [D loss: 28.791319] [G loss: 1.038213]\n",
      "[Epoch 0/1] [Batch 225/313] [D loss: 48.488674] [G loss: 1.034400]\n",
      "[Epoch 0/1] [Batch 226/313] [D loss: 34.709255] [G loss: 1.035591]\n",
      "[Epoch 0/1] [Batch 227/313] [D loss: 27.687912] [G loss: 1.036554]\n",
      "[Epoch 0/1] [Batch 228/313] [D loss: 39.191956] [G loss: 1.036665]\n",
      "[Epoch 0/1] [Batch 229/313] [D loss: 25.484512] [G loss: 1.031944]\n",
      "[Epoch 0/1] [Batch 230/313] [D loss: 34.111454] [G loss: 1.038938]\n",
      "[Epoch 0/1] [Batch 231/313] [D loss: 24.707640] [G loss: 1.032404]\n",
      "[Epoch 0/1] [Batch 232/313] [D loss: 19.102169] [G loss: 1.031429]\n",
      "[Epoch 0/1] [Batch 233/313] [D loss: 28.733873] [G loss: 1.038097]\n",
      "[Epoch 0/1] [Batch 234/313] [D loss: 38.126114] [G loss: 1.032991]\n",
      "[Epoch 0/1] [Batch 235/313] [D loss: 29.104874] [G loss: 1.030919]\n",
      "[Epoch 0/1] [Batch 236/313] [D loss: 41.139606] [G loss: 1.040211]\n",
      "[Epoch 0/1] [Batch 237/313] [D loss: 26.352749] [G loss: 1.036933]\n",
      "[Epoch 0/1] [Batch 238/313] [D loss: 23.851646] [G loss: 1.041833]\n",
      "[Epoch 0/1] [Batch 239/313] [D loss: 20.452826] [G loss: 1.034799]\n",
      "[Epoch 0/1] [Batch 240/313] [D loss: 19.137043] [G loss: 1.041114]\n",
      "[Epoch 0/1] [Batch 241/313] [D loss: 25.763510] [G loss: 1.036785]\n",
      "[Epoch 0/1] [Batch 242/313] [D loss: 37.473621] [G loss: 1.041545]\n",
      "[Epoch 0/1] [Batch 243/313] [D loss: 24.005970] [G loss: 1.038665]\n",
      "[Epoch 0/1] [Batch 244/313] [D loss: 18.705889] [G loss: 1.037353]\n",
      "[Epoch 0/1] [Batch 245/313] [D loss: 25.541380] [G loss: 1.036907]\n",
      "[Epoch 0/1] [Batch 246/313] [D loss: 28.654528] [G loss: 1.040553]\n",
      "[Epoch 0/1] [Batch 247/313] [D loss: 29.573427] [G loss: 1.029695]\n",
      "[Epoch 0/1] [Batch 248/313] [D loss: 36.858391] [G loss: 1.039229]\n",
      "[Epoch 0/1] [Batch 249/313] [D loss: 30.158560] [G loss: 1.038842]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/1] [Batch 250/313] [D loss: 29.076033] [G loss: 1.036564]\n",
      "[Epoch 0/1] [Batch 251/313] [D loss: 46.560345] [G loss: 1.034472]\n",
      "[Epoch 0/1] [Batch 252/313] [D loss: 50.194115] [G loss: 1.037705]\n",
      "[Epoch 0/1] [Batch 253/313] [D loss: 39.501266] [G loss: 1.034557]\n",
      "[Epoch 0/1] [Batch 254/313] [D loss: 30.064569] [G loss: 1.038430]\n",
      "[Epoch 0/1] [Batch 255/313] [D loss: 32.607117] [G loss: 1.041926]\n",
      "[Epoch 0/1] [Batch 256/313] [D loss: 30.807510] [G loss: 1.029407]\n",
      "[Epoch 0/1] [Batch 257/313] [D loss: 26.470297] [G loss: 1.038182]\n",
      "[Epoch 0/1] [Batch 258/313] [D loss: 20.790216] [G loss: 1.027677]\n",
      "[Epoch 0/1] [Batch 259/313] [D loss: 14.431027] [G loss: 1.035165]\n",
      "[Epoch 0/1] [Batch 260/313] [D loss: 27.175949] [G loss: 1.041269]\n",
      "[Epoch 0/1] [Batch 261/313] [D loss: 37.240044] [G loss: 1.040972]\n",
      "[Epoch 0/1] [Batch 262/313] [D loss: 16.693903] [G loss: 1.032822]\n",
      "[Epoch 0/1] [Batch 263/313] [D loss: 27.119736] [G loss: 1.034281]\n",
      "[Epoch 0/1] [Batch 264/313] [D loss: 24.597044] [G loss: 1.034145]\n",
      "[Epoch 0/1] [Batch 265/313] [D loss: 22.595007] [G loss: 1.039085]\n",
      "[Epoch 0/1] [Batch 266/313] [D loss: 21.434439] [G loss: 1.033481]\n",
      "[Epoch 0/1] [Batch 267/313] [D loss: 29.363075] [G loss: 1.037572]\n",
      "[Epoch 0/1] [Batch 268/313] [D loss: 15.744653] [G loss: 1.040268]\n",
      "[Epoch 0/1] [Batch 269/313] [D loss: 25.467171] [G loss: 1.043974]\n",
      "[Epoch 0/1] [Batch 270/313] [D loss: 28.498112] [G loss: 1.038587]\n",
      "[Epoch 0/1] [Batch 271/313] [D loss: 35.405815] [G loss: 1.035954]\n",
      "[Epoch 0/1] [Batch 272/313] [D loss: 22.319706] [G loss: 1.039154]\n",
      "[Epoch 0/1] [Batch 273/313] [D loss: 24.637077] [G loss: 1.036842]\n",
      "[Epoch 0/1] [Batch 274/313] [D loss: 34.259151] [G loss: 1.037092]\n",
      "[Epoch 0/1] [Batch 275/313] [D loss: 34.458103] [G loss: 1.038001]\n",
      "[Epoch 0/1] [Batch 276/313] [D loss: 27.729063] [G loss: 1.034297]\n",
      "[Epoch 0/1] [Batch 277/313] [D loss: 31.703503] [G loss: 1.031419]\n",
      "[Epoch 0/1] [Batch 278/313] [D loss: 25.137861] [G loss: 1.040267]\n",
      "[Epoch 0/1] [Batch 279/313] [D loss: 16.688532] [G loss: 1.037781]\n",
      "[Epoch 0/1] [Batch 280/313] [D loss: 35.316055] [G loss: 1.036343]\n",
      "[Epoch 0/1] [Batch 281/313] [D loss: 16.588041] [G loss: 1.039626]\n",
      "[Epoch 0/1] [Batch 282/313] [D loss: 24.969164] [G loss: 1.035539]\n",
      "[Epoch 0/1] [Batch 283/313] [D loss: 22.424290] [G loss: 1.037256]\n",
      "[Epoch 0/1] [Batch 284/313] [D loss: 23.714640] [G loss: 1.035583]\n",
      "[Epoch 0/1] [Batch 285/313] [D loss: 16.503170] [G loss: 1.043984]\n",
      "[Epoch 0/1] [Batch 286/313] [D loss: 14.222634] [G loss: 1.038504]\n",
      "[Epoch 0/1] [Batch 287/313] [D loss: 22.186956] [G loss: 1.032208]\n",
      "[Epoch 0/1] [Batch 288/313] [D loss: 23.480013] [G loss: 1.037440]\n",
      "[Epoch 0/1] [Batch 289/313] [D loss: 30.378012] [G loss: 1.042417]\n",
      "[Epoch 0/1] [Batch 290/313] [D loss: 16.369780] [G loss: 1.029730]\n",
      "[Epoch 0/1] [Batch 291/313] [D loss: 14.230793] [G loss: 1.040140]\n",
      "[Epoch 0/1] [Batch 292/313] [D loss: 20.093994] [G loss: 1.038246]\n",
      "[Epoch 0/1] [Batch 293/313] [D loss: 34.533466] [G loss: 1.038044]\n",
      "[Epoch 0/1] [Batch 294/313] [D loss: 14.909085] [G loss: 1.039230]\n",
      "[Epoch 0/1] [Batch 295/313] [D loss: 18.247128] [G loss: 1.026225]\n",
      "[Epoch 0/1] [Batch 296/313] [D loss: 24.972303] [G loss: 1.039913]\n",
      "[Epoch 0/1] [Batch 297/313] [D loss: 16.778986] [G loss: 1.035382]\n",
      "[Epoch 0/1] [Batch 298/313] [D loss: 21.990423] [G loss: 1.045094]\n",
      "[Epoch 0/1] [Batch 299/313] [D loss: 15.013432] [G loss: 1.038069]\n",
      "[Epoch 0/1] [Batch 300/313] [D loss: 18.873407] [G loss: 1.040444]\n",
      "[Epoch 0/1] [Batch 301/313] [D loss: 21.323294] [G loss: 1.043159]\n",
      "[Epoch 0/1] [Batch 302/313] [D loss: 29.374607] [G loss: 1.034325]\n",
      "[Epoch 0/1] [Batch 303/313] [D loss: 23.601652] [G loss: 1.028793]\n",
      "[Epoch 0/1] [Batch 304/313] [D loss: 14.731045] [G loss: 1.037338]\n",
      "[Epoch 0/1] [Batch 305/313] [D loss: 19.785889] [G loss: 1.036952]\n",
      "[Epoch 0/1] [Batch 306/313] [D loss: 15.889006] [G loss: 1.032342]\n",
      "[Epoch 0/1] [Batch 307/313] [D loss: 24.493982] [G loss: 1.033596]\n",
      "[Epoch 0/1] [Batch 308/313] [D loss: 21.067720] [G loss: 1.041307]\n",
      "[Epoch 0/1] [Batch 309/313] [D loss: 23.075342] [G loss: 1.035868]\n",
      "[Epoch 0/1] [Batch 310/313] [D loss: 18.050316] [G loss: 1.036358]\n",
      "[Epoch 0/1] [Batch 311/313] [D loss: 21.127407] [G loss: 1.038441]\n",
      "[Epoch 0/1] [Batch 312/313] [D loss: 16.663317] [G loss: 1.031346]\n",
      "[Epoch 1/1] [Batch 0/313] [D loss: 12.480001] [G loss: 1.035897]\n",
      "[Epoch 1/1] [Batch 1/313] [D loss: 19.788998] [G loss: 1.040588]\n",
      "[Epoch 1/1] [Batch 2/313] [D loss: 7.880800] [G loss: 1.037628]\n",
      "[Epoch 1/1] [Batch 3/313] [D loss: 13.552109] [G loss: 1.033285]\n",
      "[Epoch 1/1] [Batch 4/313] [D loss: 13.361133] [G loss: 1.033767]\n",
      "[Epoch 1/1] [Batch 5/313] [D loss: 18.774551] [G loss: 1.033215]\n",
      "[Epoch 1/1] [Batch 6/313] [D loss: 16.143293] [G loss: 1.034148]\n",
      "[Epoch 1/1] [Batch 7/313] [D loss: 12.010179] [G loss: 1.029364]\n",
      "[Epoch 1/1] [Batch 8/313] [D loss: 19.858154] [G loss: 1.036667]\n",
      "[Epoch 1/1] [Batch 9/313] [D loss: 15.581213] [G loss: 1.036544]\n",
      "[Epoch 1/1] [Batch 10/313] [D loss: 16.580297] [G loss: 1.032702]\n",
      "[Epoch 1/1] [Batch 11/313] [D loss: 14.387866] [G loss: 1.038504]\n",
      "[Epoch 1/1] [Batch 12/313] [D loss: 11.364299] [G loss: 1.039881]\n",
      "[Epoch 1/1] [Batch 13/313] [D loss: 14.383758] [G loss: 1.038437]\n",
      "[Epoch 1/1] [Batch 14/313] [D loss: 22.534166] [G loss: 1.044488]\n",
      "[Epoch 1/1] [Batch 15/313] [D loss: 17.562267] [G loss: 1.040940]\n",
      "[Epoch 1/1] [Batch 16/313] [D loss: 21.202877] [G loss: 1.037374]\n",
      "[Epoch 1/1] [Batch 17/313] [D loss: 14.336493] [G loss: 1.032076]\n",
      "[Epoch 1/1] [Batch 18/313] [D loss: 18.605011] [G loss: 1.042621]\n",
      "[Epoch 1/1] [Batch 19/313] [D loss: 7.768875] [G loss: 1.031962]\n",
      "[Epoch 1/1] [Batch 20/313] [D loss: 16.815599] [G loss: 1.037627]\n",
      "[Epoch 1/1] [Batch 21/313] [D loss: 28.279957] [G loss: 1.031488]\n",
      "[Epoch 1/1] [Batch 22/313] [D loss: 18.970276] [G loss: 1.036405]\n",
      "[Epoch 1/1] [Batch 23/313] [D loss: 24.250380] [G loss: 1.033201]\n",
      "[Epoch 1/1] [Batch 24/313] [D loss: 16.584785] [G loss: 1.038774]\n",
      "[Epoch 1/1] [Batch 25/313] [D loss: 21.872540] [G loss: 1.038841]\n",
      "[Epoch 1/1] [Batch 26/313] [D loss: 15.656997] [G loss: 1.036039]\n",
      "[Epoch 1/1] [Batch 27/313] [D loss: 20.146490] [G loss: 1.027862]\n",
      "[Epoch 1/1] [Batch 28/313] [D loss: 21.555645] [G loss: 1.039414]\n",
      "[Epoch 1/1] [Batch 29/313] [D loss: 17.379881] [G loss: 1.041713]\n",
      "[Epoch 1/1] [Batch 30/313] [D loss: 13.101362] [G loss: 1.034834]\n",
      "[Epoch 1/1] [Batch 31/313] [D loss: 31.520433] [G loss: 1.037996]\n",
      "[Epoch 1/1] [Batch 32/313] [D loss: 16.270168] [G loss: 1.041070]\n",
      "[Epoch 1/1] [Batch 33/313] [D loss: 21.383856] [G loss: 1.028075]\n",
      "[Epoch 1/1] [Batch 34/313] [D loss: 43.386814] [G loss: 1.040241]\n",
      "[Epoch 1/1] [Batch 35/313] [D loss: 21.897949] [G loss: 1.036453]\n",
      "[Epoch 1/1] [Batch 36/313] [D loss: 34.052441] [G loss: 1.040599]\n",
      "[Epoch 1/1] [Batch 37/313] [D loss: 22.363106] [G loss: 1.044347]\n",
      "[Epoch 1/1] [Batch 38/313] [D loss: 35.583065] [G loss: 1.039245]\n",
      "[Epoch 1/1] [Batch 39/313] [D loss: 30.943956] [G loss: 1.036680]\n",
      "[Epoch 1/1] [Batch 40/313] [D loss: 33.633274] [G loss: 1.030617]\n",
      "[Epoch 1/1] [Batch 41/313] [D loss: 21.594400] [G loss: 1.041544]\n",
      "[Epoch 1/1] [Batch 42/313] [D loss: 25.251402] [G loss: 1.039375]\n",
      "[Epoch 1/1] [Batch 43/313] [D loss: 19.258175] [G loss: 1.034538]\n",
      "[Epoch 1/1] [Batch 44/313] [D loss: 32.457283] [G loss: 1.039708]\n",
      "[Epoch 1/1] [Batch 45/313] [D loss: 25.713287] [G loss: 1.036318]\n",
      "[Epoch 1/1] [Batch 46/313] [D loss: 23.110291] [G loss: 1.028948]\n",
      "[Epoch 1/1] [Batch 47/313] [D loss: 28.038929] [G loss: 1.037918]\n",
      "[Epoch 1/1] [Batch 48/313] [D loss: 27.905657] [G loss: 1.043396]\n",
      "[Epoch 1/1] [Batch 49/313] [D loss: 28.810783] [G loss: 1.033159]\n",
      "[Epoch 1/1] [Batch 50/313] [D loss: 27.449781] [G loss: 1.043069]\n",
      "[Epoch 1/1] [Batch 51/313] [D loss: 28.591196] [G loss: 1.042717]\n",
      "[Epoch 1/1] [Batch 52/313] [D loss: 36.952362] [G loss: 1.038417]\n",
      "[Epoch 1/1] [Batch 53/313] [D loss: 14.548894] [G loss: 1.040089]\n",
      "[Epoch 1/1] [Batch 54/313] [D loss: 25.605583] [G loss: 1.040380]\n",
      "[Epoch 1/1] [Batch 55/313] [D loss: 31.985283] [G loss: 1.037550]\n",
      "[Epoch 1/1] [Batch 56/313] [D loss: 26.471535] [G loss: 1.039380]\n",
      "[Epoch 1/1] [Batch 57/313] [D loss: 25.042904] [G loss: 1.039589]\n",
      "[Epoch 1/1] [Batch 58/313] [D loss: 30.336380] [G loss: 1.036108]\n",
      "[Epoch 1/1] [Batch 59/313] [D loss: 24.812832] [G loss: 1.034428]\n",
      "[Epoch 1/1] [Batch 60/313] [D loss: 44.078934] [G loss: 1.037530]\n",
      "[Epoch 1/1] [Batch 61/313] [D loss: 27.512487] [G loss: 1.038186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1] [Batch 62/313] [D loss: 22.235720] [G loss: 1.038053]\n",
      "[Epoch 1/1] [Batch 63/313] [D loss: 25.848986] [G loss: 1.035580]\n",
      "[Epoch 1/1] [Batch 64/313] [D loss: 17.292515] [G loss: 1.032987]\n",
      "[Epoch 1/1] [Batch 65/313] [D loss: 31.837404] [G loss: 1.036259]\n",
      "[Epoch 1/1] [Batch 66/313] [D loss: 34.864513] [G loss: 1.035274]\n",
      "[Epoch 1/1] [Batch 67/313] [D loss: 36.921642] [G loss: 1.031770]\n",
      "[Epoch 1/1] [Batch 68/313] [D loss: 36.662861] [G loss: 1.035899]\n",
      "[Epoch 1/1] [Batch 69/313] [D loss: 35.850815] [G loss: 1.036577]\n",
      "[Epoch 1/1] [Batch 70/313] [D loss: 26.911283] [G loss: 1.040536]\n",
      "[Epoch 1/1] [Batch 71/313] [D loss: 22.123636] [G loss: 1.036668]\n",
      "[Epoch 1/1] [Batch 72/313] [D loss: 38.415440] [G loss: 1.040607]\n",
      "[Epoch 1/1] [Batch 73/313] [D loss: 18.656736] [G loss: 1.042263]\n",
      "[Epoch 1/1] [Batch 74/313] [D loss: 33.691460] [G loss: 1.040381]\n",
      "[Epoch 1/1] [Batch 75/313] [D loss: 33.260860] [G loss: 1.030795]\n",
      "[Epoch 1/1] [Batch 76/313] [D loss: 36.024357] [G loss: 1.033440]\n",
      "[Epoch 1/1] [Batch 77/313] [D loss: 22.267895] [G loss: 1.037131]\n",
      "[Epoch 1/1] [Batch 78/313] [D loss: 32.422607] [G loss: 1.039310]\n",
      "[Epoch 1/1] [Batch 79/313] [D loss: 24.833321] [G loss: 1.032959]\n",
      "[Epoch 1/1] [Batch 80/313] [D loss: 22.868698] [G loss: 1.038250]\n",
      "[Epoch 1/1] [Batch 81/313] [D loss: 23.992573] [G loss: 1.037044]\n",
      "[Epoch 1/1] [Batch 82/313] [D loss: 31.369179] [G loss: 1.040451]\n",
      "[Epoch 1/1] [Batch 83/313] [D loss: 35.502075] [G loss: 1.041672]\n",
      "[Epoch 1/1] [Batch 84/313] [D loss: 21.757189] [G loss: 1.041950]\n",
      "[Epoch 1/1] [Batch 85/313] [D loss: 28.769369] [G loss: 1.038202]\n",
      "[Epoch 1/1] [Batch 86/313] [D loss: 28.122908] [G loss: 1.036620]\n",
      "[Epoch 1/1] [Batch 87/313] [D loss: 38.954205] [G loss: 1.043518]\n",
      "[Epoch 1/1] [Batch 88/313] [D loss: 36.744915] [G loss: 1.039321]\n",
      "[Epoch 1/1] [Batch 89/313] [D loss: 35.494255] [G loss: 1.039165]\n",
      "[Epoch 1/1] [Batch 90/313] [D loss: 26.137094] [G loss: 1.034793]\n",
      "[Epoch 1/1] [Batch 91/313] [D loss: 34.199528] [G loss: 1.036103]\n",
      "[Epoch 1/1] [Batch 92/313] [D loss: 33.461338] [G loss: 1.032464]\n",
      "[Epoch 1/1] [Batch 93/313] [D loss: 27.655718] [G loss: 1.035869]\n",
      "[Epoch 1/1] [Batch 94/313] [D loss: 25.621508] [G loss: 1.035319]\n",
      "[Epoch 1/1] [Batch 95/313] [D loss: 38.386154] [G loss: 1.039186]\n",
      "[Epoch 1/1] [Batch 96/313] [D loss: 30.186285] [G loss: 1.042317]\n",
      "[Epoch 1/1] [Batch 97/313] [D loss: 24.453844] [G loss: 1.031536]\n",
      "[Epoch 1/1] [Batch 98/313] [D loss: 35.722179] [G loss: 1.033505]\n",
      "[Epoch 1/1] [Batch 99/313] [D loss: 34.975262] [G loss: 1.029816]\n",
      "[Epoch 1/1] [Batch 100/313] [D loss: 28.048519] [G loss: 1.033342]\n",
      "[Epoch 1/1] [Batch 101/313] [D loss: 26.511084] [G loss: 1.035326]\n",
      "[Epoch 1/1] [Batch 102/313] [D loss: 29.498755] [G loss: 1.033181]\n",
      "[Epoch 1/1] [Batch 103/313] [D loss: 42.861504] [G loss: 1.035478]\n",
      "[Epoch 1/1] [Batch 104/313] [D loss: 21.153643] [G loss: 1.034393]\n",
      "[Epoch 1/1] [Batch 105/313] [D loss: 28.671869] [G loss: 1.035752]\n",
      "[Epoch 1/1] [Batch 106/313] [D loss: 20.247524] [G loss: 1.032431]\n",
      "[Epoch 1/1] [Batch 107/313] [D loss: 22.445107] [G loss: 1.038381]\n",
      "[Epoch 1/1] [Batch 108/313] [D loss: 20.944260] [G loss: 1.034910]\n",
      "[Epoch 1/1] [Batch 109/313] [D loss: 16.864838] [G loss: 1.032198]\n",
      "[Epoch 1/1] [Batch 110/313] [D loss: 34.340572] [G loss: 1.037401]\n",
      "[Epoch 1/1] [Batch 111/313] [D loss: 26.064907] [G loss: 1.036880]\n",
      "[Epoch 1/1] [Batch 112/313] [D loss: 35.368645] [G loss: 1.040069]\n",
      "[Epoch 1/1] [Batch 113/313] [D loss: 37.622604] [G loss: 1.034489]\n",
      "[Epoch 1/1] [Batch 114/313] [D loss: 25.739521] [G loss: 1.044903]\n",
      "[Epoch 1/1] [Batch 115/313] [D loss: 23.685247] [G loss: 1.037422]\n",
      "[Epoch 1/1] [Batch 116/313] [D loss: 31.267817] [G loss: 1.039592]\n",
      "[Epoch 1/1] [Batch 117/313] [D loss: 25.547422] [G loss: 1.043024]\n",
      "[Epoch 1/1] [Batch 118/313] [D loss: 31.608494] [G loss: 1.039359]\n",
      "[Epoch 1/1] [Batch 119/313] [D loss: 25.105488] [G loss: 1.036603]\n",
      "[Epoch 1/1] [Batch 120/313] [D loss: 29.176292] [G loss: 1.042960]\n",
      "[Epoch 1/1] [Batch 121/313] [D loss: 23.291498] [G loss: 1.038064]\n",
      "[Epoch 1/1] [Batch 122/313] [D loss: 19.540306] [G loss: 1.036009]\n",
      "[Epoch 1/1] [Batch 123/313] [D loss: 16.900118] [G loss: 1.034247]\n",
      "[Epoch 1/1] [Batch 124/313] [D loss: 20.056162] [G loss: 1.042451]\n",
      "[Epoch 1/1] [Batch 125/313] [D loss: 8.917362] [G loss: 1.037476]\n",
      "[Epoch 1/1] [Batch 126/313] [D loss: 11.820064] [G loss: 1.038478]\n",
      "[Epoch 1/1] [Batch 127/313] [D loss: 5.296980] [G loss: 1.035740]\n",
      "[Epoch 1/1] [Batch 128/313] [D loss: 11.970493] [G loss: 1.034912]\n",
      "[Epoch 1/1] [Batch 129/313] [D loss: 6.731117] [G loss: 1.043014]\n",
      "[Epoch 1/1] [Batch 130/313] [D loss: 10.311385] [G loss: 1.034419]\n",
      "[Epoch 1/1] [Batch 131/313] [D loss: 8.512029] [G loss: 1.032331]\n",
      "[Epoch 1/1] [Batch 132/313] [D loss: 13.392147] [G loss: 1.043435]\n",
      "[Epoch 1/1] [Batch 133/313] [D loss: 7.287259] [G loss: 1.040438]\n",
      "[Epoch 1/1] [Batch 134/313] [D loss: 9.837219] [G loss: 1.036053]\n",
      "[Epoch 1/1] [Batch 135/313] [D loss: 13.450428] [G loss: 1.034454]\n",
      "[Epoch 1/1] [Batch 136/313] [D loss: 11.076546] [G loss: 1.043581]\n",
      "[Epoch 1/1] [Batch 137/313] [D loss: 6.193564] [G loss: 1.041455]\n",
      "[Epoch 1/1] [Batch 138/313] [D loss: 15.147005] [G loss: 1.032587]\n",
      "[Epoch 1/1] [Batch 139/313] [D loss: 12.379063] [G loss: 1.038457]\n",
      "[Epoch 1/1] [Batch 140/313] [D loss: 8.811371] [G loss: 1.034355]\n",
      "[Epoch 1/1] [Batch 141/313] [D loss: 6.045095] [G loss: 1.039805]\n",
      "[Epoch 1/1] [Batch 142/313] [D loss: 11.626233] [G loss: 1.037041]\n",
      "[Epoch 1/1] [Batch 143/313] [D loss: 8.242016] [G loss: 1.036137]\n",
      "[Epoch 1/1] [Batch 144/313] [D loss: 8.670164] [G loss: 1.030482]\n",
      "[Epoch 1/1] [Batch 145/313] [D loss: 6.200585] [G loss: 1.035579]\n",
      "[Epoch 1/1] [Batch 146/313] [D loss: 9.720517] [G loss: 1.035807]\n",
      "[Epoch 1/1] [Batch 147/313] [D loss: 10.208322] [G loss: 1.042192]\n",
      "[Epoch 1/1] [Batch 148/313] [D loss: 10.264696] [G loss: 1.037571]\n",
      "[Epoch 1/1] [Batch 149/313] [D loss: 8.308487] [G loss: 1.037710]\n",
      "[Epoch 1/1] [Batch 150/313] [D loss: 7.920026] [G loss: 1.032228]\n",
      "[Epoch 1/1] [Batch 151/313] [D loss: 7.402154] [G loss: 1.033603]\n",
      "[Epoch 1/1] [Batch 152/313] [D loss: 11.651381] [G loss: 1.036753]\n",
      "[Epoch 1/1] [Batch 153/313] [D loss: 8.440313] [G loss: 1.034794]\n",
      "[Epoch 1/1] [Batch 154/313] [D loss: 8.916378] [G loss: 1.037112]\n",
      "[Epoch 1/1] [Batch 155/313] [D loss: 7.236448] [G loss: 1.043552]\n",
      "[Epoch 1/1] [Batch 156/313] [D loss: 17.916843] [G loss: 1.027207]\n",
      "[Epoch 1/1] [Batch 157/313] [D loss: 24.972630] [G loss: 1.042035]\n",
      "[Epoch 1/1] [Batch 158/313] [D loss: 30.533718] [G loss: 1.038748]\n",
      "[Epoch 1/1] [Batch 159/313] [D loss: 23.825317] [G loss: 1.040755]\n",
      "[Epoch 1/1] [Batch 160/313] [D loss: 25.785131] [G loss: 1.034972]\n",
      "[Epoch 1/1] [Batch 161/313] [D loss: 23.131227] [G loss: 1.029257]\n",
      "[Epoch 1/1] [Batch 162/313] [D loss: 21.620119] [G loss: 1.033339]\n",
      "[Epoch 1/1] [Batch 163/313] [D loss: 21.398029] [G loss: 1.031702]\n",
      "[Epoch 1/1] [Batch 164/313] [D loss: 28.936096] [G loss: 1.033812]\n",
      "[Epoch 1/1] [Batch 165/313] [D loss: 31.257656] [G loss: 1.036873]\n",
      "[Epoch 1/1] [Batch 166/313] [D loss: 25.133360] [G loss: 1.035508]\n",
      "[Epoch 1/1] [Batch 167/313] [D loss: 35.448719] [G loss: 1.038621]\n",
      "[Epoch 1/1] [Batch 168/313] [D loss: 36.236809] [G loss: 1.041032]\n",
      "[Epoch 1/1] [Batch 169/313] [D loss: 22.395350] [G loss: 1.034647]\n",
      "[Epoch 1/1] [Batch 170/313] [D loss: 28.133801] [G loss: 1.037503]\n",
      "[Epoch 1/1] [Batch 171/313] [D loss: 31.674101] [G loss: 1.043459]\n",
      "[Epoch 1/1] [Batch 172/313] [D loss: 16.778316] [G loss: 1.040634]\n",
      "[Epoch 1/1] [Batch 173/313] [D loss: 32.133430] [G loss: 1.044221]\n",
      "[Epoch 1/1] [Batch 174/313] [D loss: 32.006218] [G loss: 1.034989]\n",
      "[Epoch 1/1] [Batch 175/313] [D loss: 26.385937] [G loss: 1.040057]\n",
      "[Epoch 1/1] [Batch 176/313] [D loss: 27.422119] [G loss: 1.037779]\n",
      "[Epoch 1/1] [Batch 177/313] [D loss: 25.692217] [G loss: 1.038134]\n",
      "[Epoch 1/1] [Batch 178/313] [D loss: 15.361114] [G loss: 1.036120]\n",
      "[Epoch 1/1] [Batch 179/313] [D loss: 21.127581] [G loss: 1.038378]\n",
      "[Epoch 1/1] [Batch 180/313] [D loss: 20.509724] [G loss: 1.036323]\n",
      "[Epoch 1/1] [Batch 181/313] [D loss: 19.368292] [G loss: 1.031160]\n",
      "[Epoch 1/1] [Batch 182/313] [D loss: 31.193956] [G loss: 1.035965]\n",
      "[Epoch 1/1] [Batch 183/313] [D loss: 35.852364] [G loss: 1.035350]\n",
      "[Epoch 1/1] [Batch 184/313] [D loss: 22.814878] [G loss: 1.034385]\n",
      "[Epoch 1/1] [Batch 185/313] [D loss: 20.561834] [G loss: 1.035962]\n",
      "[Epoch 1/1] [Batch 186/313] [D loss: 21.741980] [G loss: 1.042494]\n",
      "[Epoch 1/1] [Batch 187/313] [D loss: 17.160244] [G loss: 1.039631]\n",
      "[Epoch 1/1] [Batch 188/313] [D loss: 21.666258] [G loss: 1.032116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1] [Batch 189/313] [D loss: 24.678617] [G loss: 1.041474]\n",
      "[Epoch 1/1] [Batch 190/313] [D loss: 12.907020] [G loss: 1.035527]\n",
      "[Epoch 1/1] [Batch 191/313] [D loss: 15.127314] [G loss: 1.037707]\n",
      "[Epoch 1/1] [Batch 192/313] [D loss: 21.518711] [G loss: 1.036219]\n",
      "[Epoch 1/1] [Batch 193/313] [D loss: 18.253338] [G loss: 1.039843]\n",
      "[Epoch 1/1] [Batch 194/313] [D loss: 31.421421] [G loss: 1.038030]\n",
      "[Epoch 1/1] [Batch 195/313] [D loss: 24.601469] [G loss: 1.041583]\n",
      "[Epoch 1/1] [Batch 196/313] [D loss: 21.597473] [G loss: 1.029641]\n",
      "[Epoch 1/1] [Batch 197/313] [D loss: 14.325696] [G loss: 1.031867]\n",
      "[Epoch 1/1] [Batch 198/313] [D loss: 17.182377] [G loss: 1.036096]\n",
      "[Epoch 1/1] [Batch 199/313] [D loss: 17.502245] [G loss: 1.039093]\n",
      "[Epoch 1/1] [Batch 200/313] [D loss: 18.564356] [G loss: 1.040767]\n",
      "[Epoch 1/1] [Batch 201/313] [D loss: 15.147091] [G loss: 1.036716]\n",
      "[Epoch 1/1] [Batch 202/313] [D loss: 14.025780] [G loss: 1.039233]\n",
      "[Epoch 1/1] [Batch 203/313] [D loss: 17.772659] [G loss: 1.038437]\n",
      "[Epoch 1/1] [Batch 204/313] [D loss: 16.041609] [G loss: 1.041463]\n",
      "[Epoch 1/1] [Batch 205/313] [D loss: 25.882633] [G loss: 1.033114]\n",
      "[Epoch 1/1] [Batch 206/313] [D loss: 12.188951] [G loss: 1.047005]\n",
      "[Epoch 1/1] [Batch 207/313] [D loss: 17.195343] [G loss: 1.040949]\n",
      "[Epoch 1/1] [Batch 208/313] [D loss: 14.986168] [G loss: 1.037651]\n",
      "[Epoch 1/1] [Batch 209/313] [D loss: 22.360693] [G loss: 1.043432]\n",
      "[Epoch 1/1] [Batch 210/313] [D loss: 17.970020] [G loss: 1.045921]\n",
      "[Epoch 1/1] [Batch 211/313] [D loss: 21.758482] [G loss: 1.035669]\n",
      "[Epoch 1/1] [Batch 212/313] [D loss: 19.469660] [G loss: 1.044287]\n",
      "[Epoch 1/1] [Batch 213/313] [D loss: 22.855551] [G loss: 1.036652]\n",
      "[Epoch 1/1] [Batch 214/313] [D loss: 17.848389] [G loss: 1.042160]\n",
      "[Epoch 1/1] [Batch 215/313] [D loss: 13.935139] [G loss: 1.040214]\n",
      "[Epoch 1/1] [Batch 216/313] [D loss: 19.101553] [G loss: 1.035816]\n",
      "[Epoch 1/1] [Batch 217/313] [D loss: 20.914579] [G loss: 1.034928]\n",
      "[Epoch 1/1] [Batch 218/313] [D loss: 25.526445] [G loss: 1.034765]\n",
      "[Epoch 1/1] [Batch 219/313] [D loss: 16.034901] [G loss: 1.043293]\n",
      "[Epoch 1/1] [Batch 220/313] [D loss: 35.997936] [G loss: 1.038268]\n",
      "[Epoch 1/1] [Batch 221/313] [D loss: 30.298338] [G loss: 1.043991]\n",
      "[Epoch 1/1] [Batch 222/313] [D loss: 30.558176] [G loss: 1.044057]\n",
      "[Epoch 1/1] [Batch 223/313] [D loss: 23.971148] [G loss: 1.032700]\n",
      "[Epoch 1/1] [Batch 224/313] [D loss: 24.148092] [G loss: 1.035986]\n",
      "[Epoch 1/1] [Batch 225/313] [D loss: 29.820126] [G loss: 1.035451]\n",
      "[Epoch 1/1] [Batch 226/313] [D loss: 22.613293] [G loss: 1.035237]\n",
      "[Epoch 1/1] [Batch 227/313] [D loss: 21.674410] [G loss: 1.046524]\n",
      "[Epoch 1/1] [Batch 228/313] [D loss: 31.911924] [G loss: 1.039598]\n",
      "[Epoch 1/1] [Batch 229/313] [D loss: 19.196800] [G loss: 1.034448]\n",
      "[Epoch 1/1] [Batch 230/313] [D loss: 19.640356] [G loss: 1.034873]\n",
      "[Epoch 1/1] [Batch 231/313] [D loss: 31.188976] [G loss: 1.037171]\n",
      "[Epoch 1/1] [Batch 232/313] [D loss: 26.116304] [G loss: 1.035017]\n",
      "[Epoch 1/1] [Batch 233/313] [D loss: 25.149454] [G loss: 1.034860]\n",
      "[Epoch 1/1] [Batch 234/313] [D loss: 37.429527] [G loss: 1.035550]\n",
      "[Epoch 1/1] [Batch 235/313] [D loss: 23.795364] [G loss: 1.044137]\n",
      "[Epoch 1/1] [Batch 236/313] [D loss: 27.668440] [G loss: 1.031203]\n",
      "[Epoch 1/1] [Batch 237/313] [D loss: 29.155605] [G loss: 1.042776]\n",
      "[Epoch 1/1] [Batch 238/313] [D loss: 30.245974] [G loss: 1.034653]\n",
      "[Epoch 1/1] [Batch 239/313] [D loss: 25.290602] [G loss: 1.034820]\n",
      "[Epoch 1/1] [Batch 240/313] [D loss: 22.302160] [G loss: 1.042374]\n",
      "[Epoch 1/1] [Batch 241/313] [D loss: 22.089973] [G loss: 1.029615]\n",
      "[Epoch 1/1] [Batch 242/313] [D loss: 28.480642] [G loss: 1.029231]\n",
      "[Epoch 1/1] [Batch 243/313] [D loss: 43.538311] [G loss: 1.042087]\n",
      "[Epoch 1/1] [Batch 244/313] [D loss: 22.979488] [G loss: 1.035226]\n",
      "[Epoch 1/1] [Batch 245/313] [D loss: 36.990578] [G loss: 1.038381]\n",
      "[Epoch 1/1] [Batch 246/313] [D loss: 32.040009] [G loss: 1.041334]\n",
      "[Epoch 1/1] [Batch 247/313] [D loss: 16.151297] [G loss: 1.033590]\n",
      "[Epoch 1/1] [Batch 248/313] [D loss: 32.058784] [G loss: 1.039027]\n",
      "[Epoch 1/1] [Batch 249/313] [D loss: 26.667511] [G loss: 1.038596]\n",
      "[Epoch 1/1] [Batch 250/313] [D loss: 35.357410] [G loss: 1.033492]\n",
      "[Epoch 1/1] [Batch 251/313] [D loss: 19.803333] [G loss: 1.035628]\n",
      "[Epoch 1/1] [Batch 252/313] [D loss: 24.517443] [G loss: 1.038540]\n",
      "[Epoch 1/1] [Batch 253/313] [D loss: 29.757151] [G loss: 1.031318]\n",
      "[Epoch 1/1] [Batch 254/313] [D loss: 28.836912] [G loss: 1.048881]\n",
      "[Epoch 1/1] [Batch 255/313] [D loss: 27.647362] [G loss: 1.038581]\n",
      "[Epoch 1/1] [Batch 256/313] [D loss: 31.435263] [G loss: 1.030968]\n",
      "[Epoch 1/1] [Batch 257/313] [D loss: 37.528122] [G loss: 1.038345]\n",
      "[Epoch 1/1] [Batch 258/313] [D loss: 27.328594] [G loss: 1.036974]\n",
      "[Epoch 1/1] [Batch 259/313] [D loss: 39.775684] [G loss: 1.039884]\n",
      "[Epoch 1/1] [Batch 260/313] [D loss: 25.674555] [G loss: 1.036166]\n",
      "[Epoch 1/1] [Batch 261/313] [D loss: 38.788296] [G loss: 1.039177]\n",
      "[Epoch 1/1] [Batch 262/313] [D loss: 27.765167] [G loss: 1.042723]\n",
      "[Epoch 1/1] [Batch 263/313] [D loss: 27.909119] [G loss: 1.040132]\n",
      "[Epoch 1/1] [Batch 264/313] [D loss: 36.739769] [G loss: 1.038741]\n",
      "[Epoch 1/1] [Batch 265/313] [D loss: 34.917652] [G loss: 1.036365]\n",
      "[Epoch 1/1] [Batch 266/313] [D loss: 29.323952] [G loss: 1.035324]\n",
      "[Epoch 1/1] [Batch 267/313] [D loss: 32.982494] [G loss: 1.041160]\n",
      "[Epoch 1/1] [Batch 268/313] [D loss: 25.088066] [G loss: 1.031579]\n",
      "[Epoch 1/1] [Batch 269/313] [D loss: 36.833458] [G loss: 1.035012]\n",
      "[Epoch 1/1] [Batch 270/313] [D loss: 19.117620] [G loss: 1.041731]\n",
      "[Epoch 1/1] [Batch 271/313] [D loss: 35.180138] [G loss: 1.032888]\n",
      "[Epoch 1/1] [Batch 272/313] [D loss: 35.095417] [G loss: 1.039821]\n",
      "[Epoch 1/1] [Batch 273/313] [D loss: 32.761456] [G loss: 1.031929]\n",
      "[Epoch 1/1] [Batch 274/313] [D loss: 26.917562] [G loss: 1.038855]\n",
      "[Epoch 1/1] [Batch 275/313] [D loss: 28.776020] [G loss: 1.040320]\n",
      "[Epoch 1/1] [Batch 276/313] [D loss: 37.134460] [G loss: 1.032161]\n",
      "[Epoch 1/1] [Batch 277/313] [D loss: 22.797297] [G loss: 1.035705]\n",
      "[Epoch 1/1] [Batch 278/313] [D loss: 20.134054] [G loss: 1.035994]\n",
      "[Epoch 1/1] [Batch 279/313] [D loss: 23.731590] [G loss: 1.038644]\n",
      "[Epoch 1/1] [Batch 280/313] [D loss: 22.901995] [G loss: 1.040135]\n",
      "[Epoch 1/1] [Batch 281/313] [D loss: 23.221273] [G loss: 1.042510]\n",
      "[Epoch 1/1] [Batch 282/313] [D loss: 26.313082] [G loss: 1.034542]\n",
      "[Epoch 1/1] [Batch 283/313] [D loss: 18.291439] [G loss: 1.039068]\n",
      "[Epoch 1/1] [Batch 284/313] [D loss: 15.291968] [G loss: 1.034826]\n",
      "[Epoch 1/1] [Batch 285/313] [D loss: 13.912740] [G loss: 1.031244]\n",
      "[Epoch 1/1] [Batch 286/313] [D loss: 21.233698] [G loss: 1.034310]\n",
      "[Epoch 1/1] [Batch 287/313] [D loss: 22.705595] [G loss: 1.033456]\n",
      "[Epoch 1/1] [Batch 288/313] [D loss: 10.188943] [G loss: 1.035210]\n",
      "[Epoch 1/1] [Batch 289/313] [D loss: 16.589546] [G loss: 1.047479]\n",
      "[Epoch 1/1] [Batch 290/313] [D loss: 15.870652] [G loss: 1.036774]\n",
      "[Epoch 1/1] [Batch 291/313] [D loss: 20.969727] [G loss: 1.040891]\n",
      "[Epoch 1/1] [Batch 292/313] [D loss: 24.142567] [G loss: 1.038349]\n",
      "[Epoch 1/1] [Batch 293/313] [D loss: 30.184772] [G loss: 1.037454]\n",
      "[Epoch 1/1] [Batch 294/313] [D loss: 14.618535] [G loss: 1.034702]\n",
      "[Epoch 1/1] [Batch 295/313] [D loss: 17.180607] [G loss: 1.033556]\n",
      "[Epoch 1/1] [Batch 296/313] [D loss: 15.854816] [G loss: 1.038291]\n",
      "[Epoch 1/1] [Batch 297/313] [D loss: 20.609051] [G loss: 1.044293]\n",
      "[Epoch 1/1] [Batch 298/313] [D loss: 20.844284] [G loss: 1.036512]\n",
      "[Epoch 1/1] [Batch 299/313] [D loss: 17.636816] [G loss: 1.036600]\n",
      "[Epoch 1/1] [Batch 300/313] [D loss: 23.638065] [G loss: 1.041911]\n",
      "[Epoch 1/1] [Batch 301/313] [D loss: 20.380964] [G loss: 1.034527]\n",
      "[Epoch 1/1] [Batch 302/313] [D loss: 21.690931] [G loss: 1.034077]\n",
      "[Epoch 1/1] [Batch 303/313] [D loss: 31.251459] [G loss: 1.040895]\n",
      "[Epoch 1/1] [Batch 304/313] [D loss: 25.249004] [G loss: 1.037051]\n",
      "[Epoch 1/1] [Batch 305/313] [D loss: 22.513706] [G loss: 1.038134]\n",
      "[Epoch 1/1] [Batch 306/313] [D loss: 26.125097] [G loss: 1.036808]\n",
      "[Epoch 1/1] [Batch 307/313] [D loss: 19.897629] [G loss: 1.030969]\n",
      "[Epoch 1/1] [Batch 308/313] [D loss: 13.199595] [G loss: 1.039427]\n",
      "[Epoch 1/1] [Batch 309/313] [D loss: 15.789011] [G loss: 1.034103]\n",
      "[Epoch 1/1] [Batch 310/313] [D loss: 13.649400] [G loss: 1.030783]\n",
      "[Epoch 1/1] [Batch 311/313] [D loss: 24.221409] [G loss: 1.032669]\n",
      "[Epoch 1/1] [Batch 312/313] [D loss: 8.718195] [G loss: 1.037344]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWIAAABbCAYAAABJXo+rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3WW8fWXV7vGf3YndASYGdmF3t6BigNiFoihix4OtoGKiYneLSFmgotitWNjdrft54ef7vxf7nIfji3PW5PM59/Vmwd7/Pdda477nnNe4xjXGPNHGxkYTExMTE8vhxEt/gImJiYn/3zEvxBMTExMLY16IJyYmJhbGvBBPTExMLIx5IZ6YmJhYGPNCPDExMbEw5oV4YmJiYmHMC/HExMTEwjjpOt9s55133qjaaaedqrrHPe5R1dve9raqXvWqV1V1oxvdqKrvfve7VT3pSU/acozPfOYzVT35yU+u6uc//3lVL3rRi6r661//WtWlL33pqj74wQ9W9YAHPKCqf/zjH1Xd9KY3rWqXXXap6l//+ldV+++/f1WXuMQltrzn1772taqOOeaYqi52sYtVdfOb37yqa13rWlXttddeVe25554n+k/i8X8TD37wgzeqbnOb21R1//vfv6oXvvCFVR1xxBFVXfKSl6zqRz/6UVWPfOQjtxzjE5/4RFV77713Vd///veresUrXlHVaU5zmqrucIc7VPVf//VfVe22225V/eUvf6nqFre4RVV3uctdjvPz973vfVVd8IIX3PKeP/3pT6s67LDDqrroRS9ajb1x7Wtfu6rnPe95Ve2yyy5rj+2uu+66UWMPifHLX/7yqj772c9Wtd1221X11a9+taonPvGJW47xla98pardd9+9GrF9y1veUtWJTvTvr3X5y1++qte+9rVVPfCBD6zGPrdfxee3v/1tVW9+85uruvjFL77lPb/1rW9V9fGPf7yqK17xilXd8573PM7ntd4PfOAD1x7bqkc84hEbVdtvv31Vz3nOc6p6+tOfXtUHPvCBqq53vetV9e1vf7uqxz/+8VuO8dGPfrQa14pvfOMb1dhX9ru9+/znP7+qxz72scf5/e1ud7uqbnazm1V15jOfuar99tuvGudPjeuB64Nrjr/1+uAHP7g6/uvCZMQTExMTC+NE62xx3muvvTZq3NluectbVoMpYcoY1bbbblsNNlT1rGc9q6qTnexkVb3rXe+q6je/+U1VZz/72avBCPfYY4+qnvvc51b1s5/9rKo73/nOVfn+WN+tbnWrqq573etuec+znvWsVe28885V3fGOd6zqwhe+cFWnPOUpq/rOd75T1Ve/+tW1M4unPOUpG1VXuMIVqrrXve5V1Qte8IJqsFaMSpxe/epXbzkGtna2s52tGqzsl7/8ZTXYgWPLSjCTv//971Xd+973rurPf/5zNZgzZnDVq151y3ue5zznqQbLtF7Y92lPe9pqsI8vfvGLa4/t7rvvvlFjT9z2tret6iMf+UhVO+64YzX2zg1ucIPquIz4mc98ZjWyg3e/+93V2I9nOMMZqrrPfe5T1VOf+tRqxONvf/tbNWL7i1/8oqrXv/71Vd3kJjep6m53u9uW9zzjGc94nJ85tuzPemLOxxxzzCKMeI899tiouv71r1+NPfqyl72sGuedc/a85z1vVW94wxu2HOMxj3lMVac//emresc73lENpnu+853vOMe2J10nfvWrX1W15557VnXiE/+bo8oWXJNco2qcQ7e+9a2rkWmc61znquoPf/hDNc6fr33ta5MRT0xMTJxQsVZG/PKXv3yj6thjj63qyle+cjXY6jWvec2q/vnPf1bjzneSk5xkyzHe/va3V3WBC1ygGpoYnQbT2KwJXe5yl6uGpoa9bbXVVtVg2L/+9a+rutCFLrTlPelvRx99dFVHHXVUNe58dGgM8Ygjjlg7s9hnn302aujqV7rSlao65JBDqqFfnfzkJ6/GHf9Pf/rTlmNgeJgSfZb+duSRR1b1ve99r6rb3/721YitY2Gvl7rUpaoRWzH/3e9+t+U9v/SlL1X1hS98oaqPfexj1WBtYo1dfvzjH197bN/+9rdv1Igl5kYjpm3KjGRQf/zjH7ccQ23Dv5HtfepTn6pGbL/85S9XY99e7WpXq4aG/IMf/KAaNQwaOya4eq44pozTuqiTfPrTn66GrrrEvq3ae++9N2owSNcF+rnvKjuyN2RNNWogWKpswPksc/79739fDb3ce8lIXJu8pwzGnvX/NfbuoYceWtXnP//5aqwxbf4JT3hCVZ/85CcnI56YmJg4oWKtrglM9653vWs1NElaEEb1jGc8o6qDDjqoGtXdGs4Kupe7u7s6nRYbcKd093/oQx9aDW3N3Wvrrbc+zv/TdWpoa1//+terUfE+3elOV407tTv6EsA6VfbpuM9+9rOrwYRpxpgz7azqaU97WlWPetSjqhE7zgrHps3JQjBF+hoHyznOcY5qsBSM2JrVyDwwEyzD5/K6yvTWDUySFs6xYN/++Mc/ruqAAw6ohrYuK6mxDnROzOvFL35xNfYzhwA2dfjhh1cjtlg4hoY5y/TszRquHpkJVi47klXSR5cCNmrP0txlomK17777VsPds1prkDHRfJ0P1oTTgitKluBY4vie97ynGi4VdSzXA+dN1VWucpVqZC2ym+tc5zrV2PermdH/hMmIJyYmJhbGWhnx5z73uWroNZe5zGWqeuc731nVS17ykmpojyc96b8/3mol2J2Nd5e2SOvFThzzxje+cTV0T3fO+973vtXQed1BASusUdnHOl760pdW4+6rYk3HWwJ8k/RcftT3v//91WCxqr90zPvd735bjkFXv+ENb1gNzdg6cKbISjBDvxczseW5pMvJGPhEazBinx9DedzjHleNbMj6LQGfH1OjEb/uda+rBgPGqjBh1f4a+1b8VfXtu5/85CfV0EVV4mUE4sFX7Of23ClOcYpqML4afm2M3qvai9j626XgHFZrwTA3117E3Sv3Uo1s2s+cB5w79jaWjek6/7FtrNw5L4uzx9/4xjdueU9/K9vmgBFXPuJTn/rU/8cYTEY8MTExsTDWyojpf+4ymK4K5Q477FDVN7/5zWr4MVe73LBq+suHPvSharA4bMvdyZ0PQ+QWoNWpaGJzfLUYSo27KYaDRfo5fZlXdAl88YtfrIb+StfFrMSS1sXpQBOvwc7cwcXw4Q9/+HGOaf3EQWxV7mmp2Mk+++xTDe14tfKMKWKROp74QmU09OklIIPDinQM+t7cPva3fXzOc55zyzHsJzGyd+50pztVw9cqNte4xjWqwaD9HR/sox/96GrURrBynaU1NH6dqvz4PouscsnY1tBWT3WqU1Uj86Bxc5jIyGSgqxqx6wKoFd397nevxp7kOuFbp5O7njzoQQ+qxhpi4+9973urkVXUOOd4u50n9gFtXjZzfJiMeGJiYmJhrNVH/KMf/Wijxp2D9qbjip7pM2Ea7vY1dEn6srs8TQjT3XXXXavhO+Z4oIepFPMAqkLTUs08qPrkJz9ZjT50rINGTEvVkXbYYYetXSz+xje+sVGjOq96rqNOHLC6y172stX4bjUYklka9EZeXhkBRqLCr6oNusBU6en29PnV2NKGsTKMRaZCS/V53/CGN6w9tj/+8Y83auxTsX3KU55SDe+6Srsah5jXYHe6MWVeXBG+P0cRBmbfYouyFPMteLqtFRZWQ2/X8WXehdg6Z7DBQw45ZKkix0aNcxfz1VVIi9cB+IhHPKIaem+N+DoH3/SmN1V18MEHV0P7FQtx5/yRDepYVGPieLCWais1GPFDHvKQarByzhZrLPt8zWteM33EExMTEydUrFUjdofju6VXulPzutKIzX9YZVw0YEwYO8WesTSVVzBliRaH1fFrYhS0N7pvDd0VK8HSVG/psLp5loBqOi3Rq9jyPLuL80Ouep/pX2KFJYjtgQceWNXVr371ajBfLAMLoeti22JufoJ5GDU6n2RBH/7wh6uh/2HCvKZLgC577nOfuxpZhVjS2TfPf9BptfpvsSXVfFqlqj1GS0OmP/q9WgimbC/Kzla9y/R3urPPILOjHW+zzTb/cSz+XwBbdf7YV77jWc5ylmrsFc4FDLlGHUKXrmuJOgVGa1+ZkCe+zn/zQui8ahXqIbT7Gi4q1weZsfcwm0T/xPFhMuKJiYmJhTEvxBMTExMLY63Fujve8Y4bNYzQUnsiNwO09IooT0aoMXhHGiCV1WSgHVlKoqhH5lCoMIxe6yhjvPRcOlqj6YAFTFqphdcQEjanAw88cO1Fjx133HGjhllf+i8dlXIZdsKuJ9WtkY4ZPKMNVJFKzAwDUuSR1rGnvfKVr6xGcU+zjVRTw0cNa5LBOaQm30OspaEvetGL1h7bnXbaaaPq/Oc/fzWkGtILi6R9q42eNa2GRcs+FWNNTNrLNYWc6UxnqkZsSRNibDwAa6fiHdtVjaYP59UPf/jDajR6KGwpDL7xjW9cpFjnuqARQkGMzc7nV5BT1NNKXKNQao+6TiheeyV1KZqSMo27FF+ShXNfUXR1TdkXDR5T8PceJFiNKfvvv/8s1k1MTEycULHWYp0BJwphBHNszV1LeyEz+2qrqCYQd/fNQ2EwZHdGNjUFJT/H9ojxGkPYiFZH7G0enYdRMIMz1SvaLQExYobXNqpoqZjBmqZgw0pYg+lp2fT9FP5Y+zw2RmwVChWOMCwFDvY3hVBst0bhSzv0RS5ykWowZJmJPbEEPLLHZ1SEkxlgq4q5MqfVFly2NEVTRTmZGAucc0DjkfVRAJThyDL8ncaF1XZl6yMrVBSTyXi01+qjq5aA/aRo6DyTMTu/MGXfg02wxrB+BTR7TDwM8zGeQAGT9Uwhmp0SmzXQSqOHbL4GC7e2zjUDrGSSqw1p/xMmI56YmJhYGGvViLfffvuNGiyHzcnISuMvsR/siJWn6mEPe1g1tDNMlt5Fr2Fmp6XRGI2uo+NoA2WxwkRWR2+yrNACMTx6H9M3vfOxj33s2rW2q1/96hs17tyaJsRBvLAfd20NFDVYMqsPbYverPWZdkfPdAzjFelq2kytK3uS2K/+t1d2O58XG8ds9t1337XH9oY3vOFGDe1XDA2xN1qUvU0cVofuyw7o54b70J2Z/unOsgdZo3OBLmrfa5JhNaSn1lg3r2oxGiSwR3tl7733XkQjvvzlL79Ro16DwWsEYs3zOWXKq81IWpHVgDRsqOPIQNg2Nzc6eZiwtZX1aNoRX5lajfqK+GoO88BS9Q7Xhd13331qxBMTExMnVKxVI6apueMxPmOzb33rW6vBNDgdtCvXcEtgBJuZAW3XKzM4RuWOZ4AN3VpLI+3NA05rMEENCVg3VqPBwd1VQ8Q6gbVjZdiqwTMcHViuz7rKoNzJtb5q0XSnx4RlALRGuiddjs7pQa+bh+6vNnTQ4rgG/E7VW6xXh8mvG7IomZrqPbZl39JvaZ6rD5q0b8XfvsXkOG8wL1rkZg2dZiy2dEmZnBpBDceJc0MDh1ECsiL1Es6BdUPjg3oPzVW2bl/KDripaNxVL3zhC6uRAdLODZGyR7VPG1yFXdu76gEaxry3v3cNq3FuqZHYF9bW3nXuyc7/d5iMeGJiYmJhrFUjvuQlL7lRQztzR1Phx7ToZR6MyOlQQ7vFFPgAVT/5LzkA+GVVp1cGcFTjzond0PLoOjV0Ia4Bd0Cti5waKrMHH3zw2rW2c53rXBs1MgEamKEm9FqVaLEWhxoxk6H4PlgDVwAN1NAV7IKPVgWa5ifLwBRXx0NaT9VplXBZCF3TOr3zne9ce2y32mqrjRqM3iOSrDsHD7akbZx2XCNr2uy9xpJkbli36j4Gx4khczMIC5Pje13dtzJPGQ2maY+oj2CNRx999CIa8WUuc5mNGvUAvQCyCNkQnZz2ah1qZLCYq33tfOZcoMk7l7Wlc2pYUxma9aLZrw6sckxxlI3ITnjFtbUf3zCwyYgnJiYmFsZaNWJdP+54NCrjErFzLgqaG39qjaozLyFdRiXYnQwzpvk4JlZHv9v88FHaG22oBnvBGLHNzRVtg0aWAPbpM/JNi50MgoZG/10d1Sjz4Id298dGZQK0L9ovRsg9gRnT48XUuq/G1rpxpNDXjXvkLeVpXgK85vzu9HjZBn3XyEU6MHdNjQ46TiG6s/gbHu6RUJiYjjk/5w2WhVh3+57eu/qeYms4umNaX/t3KfBAb34Ulb0qg+aoMfRndRi8DIH7QQYhBpwXakUyCGso2+EkkUF72IO6h3OhhvZrfKy6E5eXzNLr8WEy4omJiYmFsVZGrBebnkPXpXPRWmhFmPJqhxIGrILPm4ptYaUGyGO6tEjjFDEI7A9T5jbA+laPybfIk0z7wfwccwm4C3uMj64eDIEmhllinP5dDU3ROulkcgx3dl18xppaAzHDImimshOMetWjTRs1i0HHE02ODifWS0CssFl1B9+Xlk4718UobjW0b/omLdPcBKzav+Ng8J4Ym4yBk8i5wv2y+vggTJdWyUMrQ7HeS+7bGiyUR9q5S7cVT/F17tvrNRgw9uw850qRvdLD9Q2oGdF1ed5llNbFdWLVA2/vmkNhfCdni8yQ++v4MBnxxMTExMJYKyPmt8SIVNc5HzBj+qW7m9kFNTQfx9LtRD9S4Ve5xkp0nLnT0TN5lmnO2C4troY2qNpPu9b/f9BBB1XD37gExJLu7rOZJuVurZosA1h9sKFjiK1JU7R7DARLUIVX+cdYMDBMQcyx9tWB3puHrvMPy454t7GkJYDxcjTwj9PUuRAwNxkUL2qNuNPEabv2kuxh8wM0MTF7S0zp0Gaj+DkXQA3fMH3ZQxCcb3RSPuOlwB0hs+TyUJtx3snAMGgPiajxUALs01rYyzIJ1wVxlZXzr8vM6P0YtfivdqJaIxkhfd6sFnH9T64LkxFPTExMLIy1MmIaq7sUtoZp0Wtoiir9nBI1PL1+tnkaG32GnqkqbToTBug4dClsjd8TY6nxSCdVUX/r82Fx7qJLAEvzqldetZi/lD/XtDpukhodTvRJFWWxwUpV9DfPJ6YNb35UOQas399nXIXMRsVc1Zrrg8a3BHQaYluyDRmDDE9cTOVadTDQbjE3MdL9Rj+nQZro5ucyNrHGds1jMBtXHGuwY3qo2Mo0sb8lY1vjHPWK6WKcsjxdmBwj3BM1tHXOEHMgMFjXhc1znsUE83UOiC9nhvnFst8aUxmtkWsHZ4Z9gyEfHyYjnpiYmFgYa2XE2JmOOXc+Xl46F4apgk4XruG4UN00K8IjzjEDvedYC42ONmQ6E/aiSk/Xob3V8GFufjy2J4eonqqILwF+aVo53VYMvfJmipcJcjWYPocC7yQmgtXJZDhVHMPaYFq0SevObbH69BMZCbZJ4zd3gC5L11wCvif2KVtSace6fG+M84ADDthyDOwYUzPVywM+MV2sEIvFjHnwsSvr7DjOh1XNn78VC+f7xvawPJ+TI2ndwGJ1f3J1qGtsfuCt+PIO18jWuIOcx9wmHAyyMVPaxMa+c+1xXXBe2QPqIjU8+7ogMXfnAW3b5z4+TEY8MTExsTDWyojpVxgljYc+o2rKsUAvW+2gofFgvnQZs3SxahVWx6SxqRT7e15llVi6Dx2whr7kDo19YBwYvt8vAcwSuxAz7M00L9mH2HoiQQ3NGzPm4cUWNmcPqsVYAr2Ze+Kb3/xmNZwAZk7IRmrogGYAYCA0PO9J014CXB9YK8eCZ+/R3+0hWvixxx675RhqGDRJnXVqF/Y85sVFop4iw5GF0R9lHXTT1XqKY5uJ4jyzntZnya7FGh5g+457Rw/AZu87Zi+bqOFccR5g+zJi9Q71KA4tv+fc8HtdcxxC9icXT419oe7CJWONrel/8nSZyYgnJiYmFsZap69ts802GzXYKWZp4pFJU7/+9a+rMb2KZlTD0+qJwHRmVVNTvGhE/IQYMBZAY8Mk3LV0TdGKajA57NorVuPYvIoXvehF1z7Fauutt96oUcEVOx5sOpUYY/ycIDVm4fKbcixgLLS6zSwC+6CVYnFYOI2Z7s6FUiP+5u06lveku2Gfl7vc5dYe2+22226jhudUzMRDTD0BnIaO0dWoH2DJNHHuCTNsdTqqR2BiYiub4AqiQ4rPKgvXvUcflZnQNmWTMpzzn//8i0xfu9jFLnacvUsX50LCLHmAndMYfo34yKBkhrJw2rs1NMVO5mF9PMmcawfTpkG77tRg1fau64RainqU+J75zGee09cmJiYmTqhYq0asdxwL4rfTRYUFcSGonK8+RQLjVR11B6QNY8JmHHMLmDesswkr48igb9JM/X0NJoEx6sbzfbASbgN+2nWCfsnz646OaZkYhqXS2ulcNVgy3Vx1mr6GCeqcE0OMy8wC60endzxZCCZZgy2bayET4c3VpWb2sfrCOmFKmb1k/3rSxWYNHctanTXBrWK/0YzFxjpgtF45GVTmrevmzjwZz6ov3DqYR2FvmGhH48cOscV1gzaM8Vpz+1ENQg3D9/JE8hqe6ec+97nVcLSYS8GFImtRK3EOm7BH77WXXZvUgTDmGtcca8L1Ib60eRn98c30mIx4YmJiYmGsVSM++OCDN2rolpufE6UDizZEizUxqcZd3RMmVv2+NVir+RCqzP5f5dXdVZWUZucp0byWNRwGdDlVcuxNZ5Iuv5122mntWtuhhx66UWNeqslgGALdlptCJXp1MhSWJlNRdTeNzd84ttibIIYtqORjczQ9T2RZzXCwMlPG6GxYuGPyZi7xhOwjjzzyOPtWVoGVYl3cFH5P66zBcGUA9iMd1370PbFstQ/apEyGS4gPlqtidd9i5jRMWYaY0pt9lkc+8pGLaMTHHHPMRg3vv0zYjAZOJvOddTCuOhhkb9i0OobrBPeDngDuCto6duvfyVC4UEyA9MSPGpkwdxG3DDeHLMh77rLLLlMjnpiYmDihYl6IJyYmJhbGWot1Ul6toqCxQ1om5WX/MIawRuqqEYAdzavHrhDhpXZSFqmdIod0WzrJWkdmqFHM0D7qdwovUnDWt9XHqK8L0jcplu/LxiY98tm0HbOq1Uib/RsFIJKRFFdTjAEpipmbrVZibUC5dRWnGsVTrb/SfcUptiGWMBbBdYI9bbWBoEbbvAHr0lbxMeSqRsFMbJ0DCn4KZaxl5BtFZimztSDrsU6KrXWt0brMFia99tBNTQukv1Ur4zqh+EsaU4xz3kv5FdR8Dy3lNfa7a4bCMNmQ1EBuYwywtgp/YmKwPDnHGF3XkxrFeUU4MgeJij2QzW61tX8zJiOemJiYWBhrLdZ99rOf3ahhVjeMmU2FmR1bVXhTyKgh0DuGgoMCkVfMUPFKwU3jhrsXw7a7r2E4ikVVF7rQharBPrynn7vLuhPuvPPOay96KISyL2mEUCjDNNnyFD3ZdWqwTq8YB3uO7AFT9hBQrEFsNSlg2GLL7rbKwsUQQ1GoxfiwCu+54447Ll6sw9A0E4kXBqwgbC/WKIoa0mN4j8KTV+timBVrl32qQUHLt1hi2Kv2NbG19z001CAc2Ya/WSK2VR/72Mc2algtZacGPSnayQZkXKvXBWvhusDman9juDJHzUrOE3vXo9KsnXPa9WR1gI81YQ11vogvBswCd/vb334W6yYmJiZOqFgrI95hhx02agztoGthxozQtDaWK+yghtWN9sN+pi0aU9A+TSfDjGlum8dD0oGZyt05a9xVDfHePNieFcb32G677dbOLO5zn/ts1Ljz0yLd4dmk6NwaYjC0GlY37MrYSsNsxIbNSGwNaRFbmp+9hdlsXqMaxnvsRr2ARqzRA3vbeuut1x7bRz3qURs1DPr0QpmAIUf0WftjdXyruGN17FXWS4y15npckNZceruGo83ZhmYSx6nB8jQjGMVpPayXltxtt912EUa8xx57bNT4DrRqdSBQH8B6VwccyfhksjR3e1ltSXOFv13NzmpYEdnePGDBz2nKNRixfWHvGs8pq1OvuupVrzoZ8cTExMQJFWtlxAceeOBGjRZRVVDjJI0+xDi4KFYfW6Sd1mBnjJhuhE27C2lgwE42jxNUCTe6jg5IE6rR0oxdem9aoaYEeNrTnrZ2ZkFnw5gMdb/a1a5WDc1MVRkLWm068H3EUGspzUvThcqxyrRHK2nY4NDAJsSaQ0A8Vz8vlwAmbPgPJwbt/jnPec7aY3vYYYdt1Bg+g5mp3ssQMDjff3Xfiq0BMP6tLAGb5sDRvEDjx6itp3jQkOm8fl5D35QdWR9aPw0Za3ze8563CCPef//9N2pkRfvtt1819Ft1A7FRk/F9amTTu+22WzWybY00dGZNLrR4x1B7MoCfnu69ZB7cWTUyC38jQ3Q94CaC49u7kxFPTExMLIy1MuKjjjpqo4b2qNpI18LWaEUGohhoU6OKzneqWoo58Sq7Q2oF5WV198fAMBO6E8cG7bhGazOGgW3z0RpIQ/t+/OMfvxhrwwjEyffAgLE6Ff5VXyRN0bAefl9aqFZnPk8sToungSo0cwya3qnivKoROyZmzKtsza3LCqNce2y5fbQCe8W2ZFMGyahfcI/UYJ/2nbqHvWUMo1jy/tJxMV2DhLzSJf1etlkjk8HcOFNkoLRs585ee+21CCM+/PDDN2pkYjIKWa7rhDZmrpTV6wKmq48Aq7bXPLTBOcqhZW8b2OOc9nMOKRrx6sMftDg7/51LWrSN9+XAeNzjHjcZ8cTExMQJFWvtrPP4daxVtd0IO11tPH46V+iGNe5cxl3Sxnh8aT20M//O3dRQHwOFdt1112rc+TY/Omn1GO54Picdyv9jN0uA60N1HQvFiLEKj9hRNV59ACPfNo81n7BYuvPzeXKm8AfzuMoYMGpsjXPFGtZgLvQ/XWnqBP4fy1gCYmXYE71QtZyjQyy5aTDOGg4S4xitF5cEnRnzoi/LDv2/gUtibfAU3R1brOGIwa7VU6yvY+j6Wwr8w7zUrguyJRkIh4M9o6ZUw0WiBuQhoRwYugnVhGjvMmTdb7JC3YY8wM6F1b3ruoTJywTVvmT+uv6OD5MRT0xMTCyMtWrEExMTExP/KyYjnpiYmFgY80I8MTExsTDmhXhiYmJiYcwL8cTExMTCmBfiiYl/eCDrAAAAnElEQVSJiYUxL8QTExMTC2NeiCcmJiYWxrwQT0xMTCyMeSGemJiYWBjzQjwxMTGxMOaFeGJiYmJhzAvxxMTExMKYF+KJiYmJhTEvxBMTExMLY16IJyYmJhbGvBBPTExMLIx5IZ6YmJhYGPNCPDExMbEw5oV4YmJiYmHMC/HExMTEwpgX4omJiYmFMS/EExMTEwtjXognJiYmFsZ/A3q3p61p+m81AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_gen_loss = []\n",
    "training_dis_loss = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    mean_dis_loss = []\n",
    "    mean_gen_loss = []\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "        batch_size = len(imgs)\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(FloatTensor))\n",
    "        labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
    "        gen_labels = Variable(LongTensor(np.array([1]*batch_size)))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "        \n",
    "        \n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        validity = discriminator(gen_imgs, gen_labels)\n",
    "        g_loss = adversarial_loss(validity, valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        validity_real = discriminator(real_imgs, labels)\n",
    "        d_real_loss = adversarial_loss(validity_real, valid)\n",
    "\n",
    "        # Loss for fake images\n",
    "        validity_fake = discriminator(gen_imgs.detach(), gen_labels)\n",
    "        d_fake_loss = adversarial_loss(validity_fake, fake)\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        mean_dis_loss.append(d_loss.item())\n",
    "        mean_gen_loss.append(g_loss.item())\n",
    "        \n",
    "        print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch, 1, i, len(dataloader),\n",
    "                                                            d_loss.item(), g_loss.item()))\n",
    "    \n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if epoch % 50 == 0 and batches_done % sample_interval == 0:\n",
    "            plt.figure(figsize=(5,5))\n",
    "            for k in range(4):\n",
    "                plt.subplot(4, 4, k+1)\n",
    "                plt.imshow(gen_imgs.detach().numpy()[k][0], cmap='gray')\n",
    "                plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('./images/epoch_{}_{}.png'.format(epoch+1, batches_done))\n",
    "        \n",
    "    training_gen_loss.append(np.mean(mean_dis_loss))\n",
    "    training_dis_loss.append(np.mean(mean_gen_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    torch.save({\n",
    "            'epoch': n_epochs,\n",
    "            'generator': generator.model.state_dict(),\n",
    "            'discriminator': discriminator.model.state_dict(),\n",
    "            'optimizer_gen': optimizer_G.state_dict(),\n",
    "            'optimizer_dis': optimizer_D.state_dict(),\n",
    "            'G_loss': training_gen_loss,\n",
    "            'D_loss': training_dis_loss\n",
    "            }, 'saved_models/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['saxophone',\n",
    "        'raccoon',\n",
    "        'piano',\n",
    "        'panda',\n",
    "        'leg',\n",
    "        'headphones',\n",
    "        'ceiling_fan',\n",
    "        'bed',\n",
    "        'basket',\n",
    "        'aircraft_carrier']\n",
    "\n",
    "def reshape_row(row):\n",
    "    return np.reshape(row, img_shape)\n",
    "\n",
    "all_classes = []\n",
    "for label in classes:\n",
    "    img = np.apply_along_axis(reshape_row, 1, np.load('../data/%s.npy' % label))\n",
    "    all_classes.extend([(row, label) for row in img[np.random.choice(len(img), 10)]])\n",
    "\n",
    "data = np.array(all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saxophone\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "for k in range(4):\n",
    "    plt.subplot(4, 4, k+1)\n",
    "    plt.imshow(all_classes[k][0].reshape(28, 28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./images/2.png'.format(epoch+1))\n",
    "print(all_classes[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-52b8e441643d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "data[np.random.choice(data.shape[0],32,replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('../data/panda.npy')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Quick Draw venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
